@inproceedings{kour2014real,
  title={Real-time segmentation of on-line handwritten arabic script},
  author={Kour, George and Saabne, Raid},
  booktitle={Frontiers in Handwriting Recognition (ICFHR), 2014 14th International Conference on},
  pages={417--422},
  year={2014},
  organization={IEEE}
}

@inproceedings{kour2014fast,
  title={Fast classification of handwritten on-line Arabic characters},
  author={Kour, George and Saabne, Raid},
  booktitle={Soft Computing and Pattern Recognition (SoCPaR), 2014 6th International Conference of},
  pages={312--318},
  year={2014},
  organization={IEEE}
}

@article{hadash2018estimate,
  title={Estimate and Replace: A Novel Approach to Integrating Deep Neural Networks with Existing Applications},
  author={Hadash, Guy and Kermany, Einat and Carmeli, Boaz and Lavi, Ofer and Kour, George and Jacovi, Alon},
  journal={arXiv preprint arXiv:1804.09028},
  year={2018}
}


@article{moreno-torres_unifying_2012,
	title = {A unifying view on dataset shift in classification},
	volume = {45},
	issn = {0031-3203},
	url = {http://www.sciencedirect.com/science/article/pii/S0031320311002901},
	doi = {10.1016/j.patcog.2011.06.019},
	abstract = {The field of dataset shift has received a growing amount of interest in the last few years. The fact that most real-world applications have to cope with some form of shift makes its study highly relevant. The literature on the topic is mostly scattered, and different authors use different names to refer to the same concepts, or use the same name for different concepts. With this work, we attempt to present a unifying framework through the review and comparison of some of the most important works in the literature.},
	pages = {521--530},
	number = {1},
	journaltitle = {Pattern Recognition},
	shortjournal = {Pattern Recognition},
	author = {Moreno-Torres, Jose G. and Raeder, Troy and Alaiz-Rodríguez, Rocío and Chawla, Nitesh V. and Herrera, Francisco},
	urldate = {2019-09-21},
	date = {2012-01-01},
	keywords = {Changing environments, Covariate shift, Data fracture, Dataset shift, Differing training and test populations, Non-stationary distributions, Sample selection bias},
	file = {1-s2.0-S0031320311002901-main.pdf:/Users/jiahuawu/Downloads/1-s2.0-S0031320311002901-main.pdf:application/pdf;ScienceDirect Snapshot:/Users/jiahuawu/Zotero/storage/IJ29WVMK/S0031320311002901.html:text/html}
}

@article{ilyas_adversarial_2019,
	title = {Adversarial Examples Are Not Bugs, They Are Features},
	url = {http://arxiv.org/abs/1905.02175},
	abstract = {Adversarial examples have attracted significant attention in machine learning, but the reasons for their existence and pervasiveness remain unclear. We demonstrate that adversarial examples can be directly attributed to the presence of non-robust features: features derived from patterns in the data distribution that are highly predictive, yet brittle and incomprehensible to humans. After capturing these features within a theoretical framework, we establish their widespread existence in standard datasets. Finally, we present a simple setting where we can rigorously tie the phenomena we observe in practice to a misalignment between the (human-specified) notion of robustness and the inherent geometry of the data.},
	journaltitle = {{arXiv}:1905.02175 [cs, stat]},
	author = {Ilyas, Andrew and Santurkar, Shibani and Tsipras, Dimitris and Engstrom, Logan and Tran, Brandon and Madry, Aleksander},
	urldate = {2019-09-21},
	date = {2019-05-06},
	eprinttype = {arxiv},
	eprint = {1905.02175},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv\:1905.02175 PDF:/Users/jiahuawu/Zotero/storage/GCA2ZGG2/Ilyas et al. - 2019 - Adversarial Examples Are Not Bugs, They Are Featur.pdf:application/pdf;arXiv.org Snapshot:/Users/jiahuawu/Zotero/storage/Q2ETGQ88/1905.html:text/html}
}

@article{kapishnikov_xrai:_2019,
	title = {{XRAI}: Better Attributions Through Regions},
	url = {http://arxiv.org/abs/1906.02825},
	shorttitle = {{XRAI}},
	abstract = {Saliency methods can aid understanding of deep neural networks. Recent years have witnessed many improvements to saliency methods, as well as new ways for evaluating them. In this paper, we 1) present a novel region-based attribution method, {XRAI}, that builds upon integrated gradients (Sundararajan et al. 2017), 2) introduce evaluation methods for empirically assessing the quality of image-based saliency maps (Performance Information Curves ({PICs})), and 3) contribute an axiom-based sanity check for attribution methods. Through empirical experiments and example results, we show that {XRAI} produces better results than other saliency methods for common models and the {ImageNet} dataset.},
	journaltitle = {{arXiv}:1906.02825 [cs, stat]},
	author = {Kapishnikov, Andrei and Bolukbasi, Tolga and Viégas, Fernanda and Terry, Michael},
	urldate = {2019-09-21},
	date = {2019-06-06},
	eprinttype = {arxiv},
	eprint = {1906.02825},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
	file = {arXiv\:1906.02825 PDF:/Users/jiahuawu/Zotero/storage/LDGSGLTR/Kapishnikov et al. - 2019 - XRAI Better Attributions Through Regions.pdf:application/pdf;arXiv.org Snapshot:/Users/jiahuawu/Zotero/storage/MN4WBZPE/1906.html:text/html}
}

@article{sundararajan_axiomatic_2017,
	title = {Axiomatic Attribution for Deep Networks},
	url = {http://arxiv.org/abs/1703.01365},
	abstract = {We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms---Sensitivity and Implementation Invariance that attribution methods ought to satisfy. We show that they are not satisfied by most known attribution methods, which we consider to be a fundamental weakness of those methods. We use the axioms to guide the design of a new attribution method called Integrated Gradients. Our method requires no modification to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator. We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network, and to enable users to engage with models better.},
	journaltitle = {{arXiv}:1703.01365 [cs]},
	author = {Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
	urldate = {2019-09-21},
	date = {2017-03-03},
	eprinttype = {arxiv},
	eprint = {1703.01365},
	keywords = {Computer Science - Machine Learning},
	file = {1703.01365.pdf:/Users/jiahuawu/Downloads/1703.01365.pdf:application/pdf;arXiv\:1703.01365 PDF:/Users/jiahuawu/Zotero/storage/X7Z86SRC/Sundararajan et al. - 2017 - Axiomatic Attribution for Deep Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/jiahuawu/Zotero/storage/QCFRSJKF/1703.html:text/html}
}

@article{ancona_explaining_2019,
	title = {Explaining Deep Neural Networks with a Polynomial Time Algorithm for Shapley Values Approximation},
	url = {http://arxiv.org/abs/1903.10992},
	abstract = {The problem of explaining the behavior of deep neural networks has recently gained a lot of attention. While several attribution methods have been proposed, most come without strong theoretical foundations, which raises questions about their reliability. On the other hand, the literature on cooperative game theory suggests Shapley values as a unique way of assigning relevance scores such that certain desirable properties are satisfied. Unfortunately, the exact evaluation of Shapley values is prohibitively expensive, exponential in the number of input features. In this work, by leveraging recent results on uncertainty propagation, we propose a novel, polynomial-time approximation of Shapley values in deep neural networks. We show that our method produces significantly better approximations of Shapley values than existing state-of-the-art attribution methods.},
	journaltitle = {{arXiv}:1903.10992 [cs, stat]},
	author = {Ancona, Marco and Öztireli, Cengiz and Gross, Markus},
	urldate = {2019-09-21},
	date = {2019-03-26},
	eprinttype = {arxiv},
	eprint = {1903.10992},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv\:1903.10992 PDF:/Users/jiahuawu/Zotero/storage/9RG4CEUX/Ancona et al. - 2019 - Explaining Deep Neural Networks with a Polynomial .pdf:application/pdf;arXiv.org Snapshot:/Users/jiahuawu/Zotero/storage/D2EEJIWY/1903.html:text/html}
}

@online{noauthor_tricking_nodate,
	title = {Tricking Neural Networks: Create your own Adversarial Examples},
	url = {https://medium.com/@ml.at.berkeley/tricking-neural-networks-create-your-own-adversarial-examples-a61eb7620fd8}
}

@article{szegedy_rethinking_2015,
	title = {Rethinking the Inception Architecture for Computer Vision},
	url = {http://arxiv.org/abs/1512.00567},
	abstract = {Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the {ILSVRC} 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2\% top-1 and 5.6\% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5\% top-5 error on the validation set (3.6\% error on the test set) and 17.3\% top-1 error on the validation set.},
	journaltitle = {{arXiv}:1512.00567 [cs]},
	author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jonathon and Wojna, Zbigniew},
	urldate = {2019-09-29},
	date = {2015-12-01},
	eprinttype = {arxiv},
	eprint = {1512.00567},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1512.00567 PDF:/Users/jiahuawu/Zotero/storage/JX5QGEJ9/Szegedy et al. - 2015 - Rethinking the Inception Architecture for Computer.pdf:application/pdf;arXiv.org Snapshot:/Users/jiahuawu/Zotero/storage/HATLAC59/1512.html:text/html}
}

@article{kurakin_adversarial_2016,
	title = {Adversarial Machine Learning at Scale},
	url = {http://arxiv.org/abs/1611.01236},
	abstract = {Adversarial examples are malicious inputs designed to fool machine learning models. They often transfer from one model to another, allowing attackers to mount black box attacks without knowledge of the target model's parameters. Adversarial training is the process of explicitly training a model on adversarial examples, in order to make it more robust to attack or to reduce its test error on clean inputs. So far, adversarial training has primarily been applied to small problems. In this research, we apply adversarial training to {ImageNet}. Our contributions include: (1) recommendations for how to succesfully scale adversarial training to large models and datasets, (2) the observation that adversarial training confers robustness to single-step attack methods, (3) the finding that multi-step attack methods are somewhat less transferable than single-step attack methods, so single-step attacks are the best for mounting black-box attacks, and (4) resolution of a "label leaking" effect that causes adversarially trained models to perform better on adversarial examples than on clean examples, because the adversarial example construction process uses the true label and the model can learn to exploit regularities in the construction process.},
	journaltitle = {{arXiv}:1611.01236 [cs, stat]},
	author = {Kurakin, Alexey and Goodfellow, Ian and Bengio, Samy},
	urldate = {2019-09-29},
	date = {2016-11-03},
	eprinttype = {arxiv},
	eprint = {1611.01236},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv\:1611.01236 PDF:/Users/jiahuawu/Zotero/storage/NP327CGA/Kurakin et al. - 2016 - Adversarial Machine Learning at Scale.pdf:application/pdf;arXiv.org Snapshot:/Users/jiahuawu/Zotero/storage/LNCGAB7E/1611.html:text/html}
}

@article{brendel_decision-based_2017,
	title = {Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models},
	url = {http://arxiv.org/abs/1712.04248},
	shorttitle = {Decision-Based Adversarial Attacks},
	abstract = {Many machine learning algorithms are vulnerable to almost imperceptible perturbations of their inputs. So far it was unclear how much risk adversarial perturbations carry for the safety of real-world machine learning applications because most methods used to generate such perturbations rely either on detailed model information (gradient-based attacks) or on confidence scores such as class probabilities (score-based attacks), neither of which are available in most real-world scenarios. In many such cases one currently needs to retreat to transfer-based attacks which rely on cumbersome substitute models, need access to the training data and can be defended against. Here we emphasise the importance of attacks which solely rely on the final model decision. Such decision-based attacks are (1) applicable to real-world black-box models such as autonomous cars, (2) need less knowledge and are easier to apply than transfer-based attacks and (3) are more robust to simple defences than gradient- or score-based attacks. Previous attacks in this category were limited to simple models or simple datasets. Here we introduce the Boundary Attack, a decision-based attack that starts from a large adversarial perturbation and then seeks to reduce the perturbation while staying adversarial. The attack is conceptually simple, requires close to no hyperparameter tuning, does not rely on substitute models and is competitive with the best gradient-based attacks in standard computer vision tasks like {ImageNet}. We apply the attack on two black-box algorithms from Clarifai.com. The Boundary Attack in particular and the class of decision-based attacks in general open new avenues to study the robustness of machine learning models and raise new questions regarding the safety of deployed machine learning systems. An implementation of the attack is available as part of Foolbox at https://github.com/bethgelab/foolbox .},
	journaltitle = {{arXiv}:1712.04248 [cs, stat]},
	author = {Brendel Wieland and Rauber Jonas and Bethge Matthias},
	urldate = {2019-09-29},
	date = {2017-12-12},
	eprinttype = {arxiv},
	eprint = {1712.04248},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv\:1712.04248 PDF:/Users/jiahuawu/Zotero/storage/28PKF6BH/Brendel et al. - 2017 - Decision-Based Adversarial Attacks Reliable Attac.pdf:application/pdf;arXiv.org Snapshot:/Users/jiahuawu/Zotero/storage/DRAZIAZT/1712.html:text/html}
}

@article{papernot_distillation_2015,
	title = {Distillation as a Defense to Adversarial Perturbations against Deep Neural Networks},
	url = {http://arxiv.org/abs/1511.04508},
	abstract = {Deep learning algorithms have been shown to perform extremely well on many classical machine learning problems. However, recent studies have shown that deep learning, like other machine learning techniques, is vulnerable to adversarial samples: inputs crafted to force a deep neural network ({DNN}) to provide adversary-selected outputs. Such attacks can seriously undermine the security of the system supported by the {DNN}, sometimes with devastating consequences. For example, autonomous vehicles can be crashed, illicit or illegal content can bypass content filters, or biometric authentication systems can be manipulated to allow improper access. In this work, we introduce a defensive mechanism called defensive distillation to reduce the effectiveness of adversarial samples on {DNNs}. We analytically investigate the generalizability and robustness properties granted by the use of defensive distillation when training {DNNs}. We also empirically study the effectiveness of our defense mechanisms on two {DNNs} placed in adversarial settings. The study shows that defensive distillation can reduce effectiveness of sample creation from 95\% to less than 0.5\% on a studied {DNN}. Such dramatic gains can be explained by the fact that distillation leads gradients used in adversarial sample creation to be reduced by a factor of 10{\textasciicircum}30. We also find that distillation increases the average minimum number of features that need to be modified to create adversarial samples by about 800\% on one of the {DNNs} we tested.},
	journaltitle = {{arXiv}:1511.04508 [cs, stat]},
	author = {Papernot, Nicolas and {McDaniel}, Patrick and Wu, Xi and Jha, Somesh and Swami, Ananthram},
	urldate = {2019-09-30},
	date = {2015-11-13},
	eprinttype = {arxiv},
	eprint = {1511.04508},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv\:1511.04508 PDF:/Users/jiahuawu/Zotero/storage/FVYUXWLQ/Papernot et al. - 2015 - Distillation as a Defense to Adversarial Perturbat.pdf:application/pdf;arXiv.org Snapshot:/Users/jiahuawu/Zotero/storage/H343YFJ3/1511.html:text/html}
}

@article{madry_towards_2017,
	title = {Towards Deep Learning Models Resistant to Adversarial Attacks},
	url = {http://arxiv.org/abs/1706.06083},
	abstract = {Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at https://github.com/{MadryLab}/mnist\_challenge and https://github.com/{MadryLab}/cifar10\_challenge.},
	journaltitle = {{arXiv}:1706.06083 [cs, stat]},
	author = {Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
	urldate = {2019-10-16},
	date = {2017-06-19},
	eprinttype = {arxiv},
	eprint = {1706.06083},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv\:1706.06083 PDF:/Users/jiahuawu/Zotero/storage/3KF2HD8B/Madry et al. - 2017 - Towards Deep Learning Models Resistant to Adversar.pdf:application/pdf;arXiv.org Snapshot:/Users/jiahuawu/Zotero/storage/WCNLGKXE/1706.html:text/html}
}

@article{noauthor_ascent_2019,
    author = {Nature Editorial},
	title = {Ascent of machine learning in medicine},
	volume = {18},
	issn = {1476-1122, 1476-4660},
	url = {http://www.nature.com/articles/s41563-019-0360-1},
	doi = {10.1038/s41563-019-0360-1},
	pages = {407--407},
	number = {5},
	journaltitle = {Nature Materials},
	shortjournal = {Nat. Mater.},
	urldate = {2019-10-27},
	date = {2019-05},
	langid = {english},
	file = {2019 - Ascent of machine learning in medicine.pdf:/Users/jiahuawu/Zotero/storage/KGKI9IIL/2019 - Ascent of machine learning in medicine.pdf:application/pdf}
}

@article{finlayson_adversarial_2019,
	title = {Adversarial attacks on medical machine learning},
	volume = {363},
	rights = {Copyright © 2019, American Association for the Advancement of Science. http://www.sciencemag.org/about/science-licenses-journal-article-{reuseThis} is an article distributed under the terms of the Science Journals Default License.},
	issn = {0036-8075, 1095-9203},
	url = {https://science.sciencemag.org/content/363/6433/1287},
	doi = {10.1126/science.aaw4399},
	abstract = {Emerging vulnerabilities demand new conversations
Emerging vulnerabilities demand new conversations},
	pages = {1287--1289},
	number = {6433},
	journaltitle = {Science},
	author = {Finlayson, Samuel G. and Bowers, John D. and Ito, Joichi and Zittrain, Jonathan L. and Beam, Andrew L. and Kohane, Isaac S.},
	urldate = {2019-10-27},
	date = {2019-03-22},
	langid = {english},
	pmid = {30898923},
	file = {Full Text PDF:/Users/jiahuawu/Zotero/storage/C3GVQ2K5/Finlayson et al. - 2019 - Adversarial attacks on medical machine learning.pdf:application/pdf;Snapshot:/Users/jiahuawu/Zotero/storage/YQXABZWL/1287.html:text/html}
}

@article{teschendorff_avoiding_2019,
	title = {Avoiding common pitfalls in machine learning omic data science},
	volume = {18},
	rights = {2018 Springer Nature Limited},
	issn = {1476-4660},
	url = {https://www.nature.com/articles/s41563-018-0241-z},
	doi = {10.1038/s41563-018-0241-z},
	abstract = {This Comment describes some of the common pitfalls encountered in deriving and validating predictive statistical models from high-dimensional data. It offers a fresh perspective on some key statistical issues, providing some guidelines to avoid pitfalls, and to help unfamiliar readers better assess the reliability and significance of their results.},
	pages = {422--427},
	number = {5},
	journaltitle = {Nature Materials},
	shortjournal = {Nat. Mater.},
	author = {Teschendorff, Andrew E.},
	urldate = {2019-10-27},
	date = {2019-05},
	langid = {english},
	file = {Full Text PDF:/Users/jiahuawu/Zotero/storage/6MX6F8FN/Teschendorff - 2019 - Avoiding common pitfalls in machine learning omic .pdf:application/pdf;Snapshot:/Users/jiahuawu/Zotero/storage/8A5VZSLY/s41563-018-0241-z.html:text/html}
}

@article{doan_leveraging_2019,
	title = {Leveraging machine vision in cell-based diagnostics to do more with less},
	volume = {18},
	rights = {2019 Springer Nature Limited},
	issn = {1476-4660},
	url = {https://www.nature.com/articles/s41563-019-0339-y},
	doi = {10.1038/s41563-019-0339-y},
	abstract = {Highly quantitative, robust, single-cell analyses can help to unravel disease heterogeneity and lead to clinical insights, particularly for complex and chronic diseases. Advances in computer vision and machine learning can empower label-free cell-based diagnostics to capture subtle disease states.},
	pages = {414--418},
	number = {5},
	journaltitle = {Nature Materials},
	shortjournal = {Nat. Mater.},
	author = {Doan, Minh and Carpenter, Anne E.},
	urldate = {2019-10-27},
	date = {2019-05},
	langid = {english},
	file = {Full Text PDF:/Users/jiahuawu/Zotero/storage/945LT6GT/Doan and Carpenter - 2019 - Leveraging machine vision in cell-based diagnostic.pdf:application/pdf;Snapshot:/Users/jiahuawu/Zotero/storage/ERPWDISG/s41563-019-0339-y.html:text/html}
}

@article{chen_how_2019,
	title = {How to develop machine learning models for healthcare},
	volume = {18},
	rights = {2019 Springer Nature Limited},
	issn = {1476-4660},
	url = {https://www.nature.com/articles/s41563-019-0345-0},
	doi = {10.1038/s41563-019-0345-0},
	abstract = {Rapid progress in machine learning is enabling opportunities for improved clinical decision support. Importantly, however, developing, validating and implementing machine learning models for healthcare entail some particular considerations to increase the chances of eventually improving patient care.},
	pages = {410--414},
	number = {5},
	journaltitle = {Nature Materials},
	shortjournal = {Nat. Mater.},
	author = {Chen, Po-Hsuan Cameron and Liu, Yun and Peng, Lily},
	urldate = {2019-10-27},
	date = {2019-05},
	langid = {english},
	file = {Full Text PDF:/Users/jiahuawu/Zotero/storage/WRCD4JHQ/Chen et al. - 2019 - How to develop machine learning models for healthc.pdf:application/pdf;Snapshot:/Users/jiahuawu/Zotero/storage/YQ7INULZ/s41563-019-0345-0.html:text/html}
}

@article{adv2,
  author    = {Battista Biggio and
               Igino Corona and
               Davide Maiorca and
               Blaine Nelson and
               Nedim Srndic and
               Pavel Laskov and
               Giorgio Giacinto and
               Fabio Roli},
  title     = {Evasion Attacks against Machine Learning at Test Time},
  journal   = {CoRR},
  volume    = {abs/1708.06131},
  year      = {2017},
  url       = {http://arxiv.org/abs/1708.06131},
  archivePrefix = {arXiv},
  eprint    = {1708.06131},
  timestamp = {Mon, 13 Aug 2018 16:48:20 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1708-06131},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{adv1,
title	= {Intriguing properties of neural networks},
author	= {Christian Szegedy and Wojciech Zaremba and Ilya Sutskever and Joan Bruna and Dumitru Erhan and Ian Goodfellow and Rob Fergus},
year	= {2014},
URL	= {http://arxiv.org/abs/1312.6199},
booktitle	= {International Conference on Learning Representations}
}

@inproceedings{fgsm,
title	= {Explaining and Harnessing Adversarial Examples},
author	= {Ian Goodfellow and Jonathon Shlens and Christian Szegedy},
year	= {2015},
URL	= {http://arxiv.org/abs/1412.6572},
booktitle	= {International Conference on Learning Representations}
}

@article{lin_focal_2018,
	title = {Focal Loss for Dense Object Detection},
	url = {http://arxiv.org/abs/1708.02002},
	journaltitle = {{arXiv}:1708.02002 [cs]},
	author = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Dollár, Piotr},
	urldate = {2020-01-08},
	date = {2018-02-07},
	eprinttype = {arxiv},
	eprint = {1708.02002},
	keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{cui_class-balanced_2019,
	title = {Class-Balanced Loss Based on Effective Number of Samples},
	url = {http://arxiv.org/abs/1901.05555},
	journaltitle = {{arXiv}:1901.05555 [cs]},
	author = {Cui, Yin and Jia, Menglin and Lin, Tsung-Yi and Song, Yang and Belongie, Serge},
	urldate = {2020-01-08},
	date = {2019-01-16},
	eprinttype = {arxiv},
	eprint = {1901.05555},
	keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{szegedy_rethinking_2015,
	title = {Rethinking the Inception Architecture for Computer Vision},
	url = {http://arxiv.org/abs/1512.00567},
	journaltitle = {{arXiv}:1512.00567 [cs]},
	author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jonathon and Wojna, Zbigniew},
	urldate = {2020-01-08},
	date = {2015-12-11},
	eprinttype = {arxiv},
	eprint = {1512.00567},
	keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{tan_efficientnet_2019,
	title = {{EfficientNet}: Rethinking Model Scaling for Convolutional Neural Networks},
	url = {http://arxiv.org/abs/1905.11946},
	shorttitle = {{EfficientNet}},
	journaltitle = {{arXiv}:1905.11946 [cs, stat]},
	author = {Tan, Mingxing and Le, Quoc V.},
	urldate = {2020-01-08},
	date = {2019-11-22},
	eprinttype = {arxiv},
	eprint = {1905.11946},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning}
}

@article{he_deep_2015,
	title = {Deep Residual Learning for Image Recognition},
	url = {http://arxiv.org/abs/1512.03385},
	journaltitle = {{arXiv}:1512.03385 [cs]},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	urldate = {2020-01-08},
	date = {2015-12-10},
	eprinttype = {arxiv},
	eprint = {1512.03385},
	keywords = {Computer Science - Computer Vision and Pattern Recognition}
}